{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import heapq\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import colors\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/review.csv',nrows=10000)\n",
    "dfUser = pd.read_csv('./data/user.csv')\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "# matrix is a n* 2000+ matrx, with each words represented by integer\n",
    "matrix = vectorizer.fit_transform(df.text)\n",
    "\n",
    "X = matrix\n",
    "y = df.stars.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\tadaboost for 1 and 5 star\n",
    "''' \n",
    "df15 = df[((df.stars == 1)| (df.stars == 5))]\n",
    "df15.index = range(len(df15))\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "# matrix is a n* 2000+ matrx, with each words represented by integer\n",
    "matrix = vectorizer.fit_transform(df15.text)\n",
    "\n",
    "\n",
    "X = matrix\n",
    "y = df15.stars.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200,learning_rate = 0.5)\n",
    "bdt.fit(X_train, y_train)\n",
    "\n",
    "z = bdt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaboost15\n",
      "error: 114\n",
      "total test: 1486\n"
     ]
    }
   ],
   "source": [
    "print 'adaboost15'\n",
    "print 'error: {}'.format(sum(z!=y_test))\n",
    "print 'total test: {}'.format(len(y_test))\n",
    "\n",
    "# Y = bdt.feature_importances_\n",
    "# Y = Y[Y!=0]\n",
    "# N = len(Y)\n",
    "# #index = [i for i, e in enumerate(Y) if e != 0]\n",
    "# #print index\n",
    "# X = range(N)\n",
    "# plt.bar(X,Y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = vectorizer.vocabulary_ # vocab of all appeared words in corpus\n",
    "importances = bdt.feature_importances_\n",
    "index = range(len(importances))\n",
    "count = np.array(matrix.sum(axis=0))[0]\n",
    "length = np.array(X_test.sum(axis=1))\n",
    "length = [item for sublist in length for item in sublist]\n",
    "\n",
    "vocabNew = dict((value, key) for (key, value) in vocab.iteritems())\n",
    "\n",
    "\n",
    "wordImptDict = {}\n",
    "imptWordDict = {}\n",
    "frequent = {}\n",
    "#for value in index:\n",
    "for ind,word in vocabNew.iteritems():\n",
    "        wordImptDict[word] = importances[ind]\n",
    "        frequent[word] = count[ind]\n",
    "\n",
    "nMostImpt = 150\n",
    "\n",
    "words = heapq.nlargest(nMostImpt, wordImptDict, key = wordImptDict.get)\n",
    "words = [word.encode() for word in words] # array of important words\n",
    "impt = [wordImptDict[word] for word in words  ] # array of importances \n",
    "cot = [frequent[word] for word in words] # array of counts of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot bar plot of importances against words\n",
    "plt.figure()\n",
    "plt.title(\"Word relative importances calculated by adaboost\")\n",
    "#plt.xticks(range(len(impt)),words)\n",
    "plt.bar(range(len(impt)), impt, color = \"palegoldenrod\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot bar plot of importances against words\n",
    "plt.figure()\n",
    "plt.title(\"Word frequency in reviews\")\n",
    "#plt.xticks(range(len(cot)), words)\n",
    "plt.bar(range(len(cot)), cot,color = \"palegoldenrod\")\n",
    "plt.plot([0, 160], [np.mean(cot), np.mean(cot)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot part of speech allocation\n",
    "# nltk.download()\n",
    "tagged = nltk.pos_tag(words)\n",
    "tags = [t for (w,t) in tagged]\n",
    "letter_counts = Counter(tags)\n",
    "dfWords = pd.DataFrame.from_dict(letter_counts, orient='index')\n",
    "dfWords.plot(kind=\"bar\",rot=0,color = \"goldenrod\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'str'>\n"
     ]
    }
   ],
   "source": [
    "## Use the first 500 cause there's some unknown error in one entry between 500 and 1000\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tex = [tokenizer.tokenize(sentence) for sentence in df.text[range(500)]]\n",
    "tex = [item for sublist in tex for item in sublist]\n",
    "\n",
    "print type(tex[0])\n",
    "tagged = nltk.pos_tag(tex[0:500])\n",
    "tags = [t for (w,t) in tagged]\n",
    "letter_counts = Counter(tags)\n",
    "dfWords = pd.DataFrame.from_dict(letter_counts, orient='index')\n",
    "dfWords.plot(kind=\"bar\",rot=0,color = \"palegoldenrod\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1486\n",
      "1486\n"
     ]
    }
   ],
   "source": [
    "## Error analysis\n",
    "# print length\n",
    "length = pd.DataFrame(length,columns = ['length of review'])\n",
    "length['Categories'] = (z!=y_test)\n",
    "#pd.options.display.mpl_style = 'default'\n",
    "length.boxplot(by='Categories')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
